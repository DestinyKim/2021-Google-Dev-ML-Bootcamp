{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression as a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/k9UzJm7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression은 Classification에서 많이 사용.\n",
    "* 위의 사진은 고양이 사진인지 분류하는 Binary Classification 문제임.\n",
    "  사진은 64*64*3 픽셀로 이루어짐. 이 모든 픽셀 값들을 이용하여 Logistic Regression(LR) 모델을 만들기 위해 Input 데이터의 Shape은 열벡터이어야 하고, 따라서, 픽셀값들을 다 곱하여 수평으로 나열한다. 이렇게 만든 X는 12288차원의 열벡터가 되고, 이를 입력하여 LR을 통해 Y를 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/4NiRs5j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 입력값 X는 nx차원의 열벡터이고, y는 1 또는 0(고양이라면 1, 아니면 0)으로 표현.  \n",
    "    * m개의 training example이 있으므로 m개의 (x(i), y(i)) 데이터가 존재.\n",
    "    * X는 m개의 x(i)를 수평으로 묶은 벡터, 따라서, X는 nx*b(nx, b)의 shape을 가짐\n",
    "    * Y는 m개의 y(i)를 수평으로 묶은 행벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/GFi5sJZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LR에서 구하고자 하는 값은 y hat(예측값). 즉, y가 1이 될 확률\n",
    "* LR에는 두 개의 Parameter W와 b 존재. 따라서, 주어진 데이터에서 좋은 결과를 예측하기 위해서는 W와 b를 업데이트 해줘야 함.\n",
    "* w는 x와 같은 차원의 열벡터, b는 상수.\n",
    "* w와 x를 내적(wT.x)하고 b를 더한 값은 0과 1사이의 값이 아닐 수 있음. sigmoid 함수를 사용하여 이 값을 0과 1사이의 값으로 만든다.\n",
    "* igmoid함수는 σ(x)라 쓰고, 식은 위의 사진에 나와 있음. 증가함수이며, (0, 0.5)에 대칭이고, y값은 x가 작아지면 0에 근사하고, 커지면 1에 근사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR:cost function & loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/xUJMlcD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* x값을 입력하여 예측값이 y hat을 구하면 실제 y값과의 차이(error)가 발생되는데, 이것이 cost function과 loss function이다. 따라서, 이 함수의 값이 작으면 작을수록 LR의 성능이 좋다고 할 수 있다. 그래서 w, b를 update 하여 cost function을 최소화 하도록 함.\n",
    "* loss function은 하나의 데이터에 대한 error이고, cost function은 전체(m개의)의 training date에 대한 error로, 모든 데이터의 loss function의 평균이라 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/hb8pyK0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위를 보면 이러한 cost function의 값이 작을수록 error가 작아지는지 확인할 수 있다. y가 1이라면 y hat이 1에 가까워 져야하고, y가 0이라면 y hat은 0에 가까워져야 한다.(LR은 지도학습이므로 y는 주어진다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/28Dowoj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient Descent는 w, b를 update 하는 방법이다. 여기에서 w, b를 최소화 하는 함수를 찾아야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/hXG3aKl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cost function을 2차원으로 표현한 것으로, Gradient는 경사라 생각하면 된다.\n",
    "* 특정 w일 때의 J(w, b)의 경사(기울기)를 구하고 그 경사를 따라 내려오도록 w를 update 한다. b도 마찬가지로 update한다. 이것을 Gradient Descent(최대경사하강법)이라 일컬음.\n",
    "* 특정 parameter에서 cost function의 minimum으로 가는 가장 가파른 경사를 갖는 길을 찾고, 그 길을 따라 (Learning rate - α)만큼 내려가는 (w, b를 update) 것.\n",
    "* 최대 경사 구하는 법 : J(w, b)를 w와 b로 편미분**(여기서 기억해야 할 것은 J(w, b)를 w로 편미분한 것(경사)를 dw, b로 편미분 한것을 db같이 표기. z로 편미분 하면 dz)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/9bVzSOH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dw(i) = x(i)dz, db = dz ( 아래 그림 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/kp5YtFT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/SrsBkdo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss를 평균내면 cost. 위 그림 참고!!! 즉 cost를 미분한 것은, 각 데이터별로 loss의 경사(gradient)를 구해서(미분) 평균을 내면 됨. 즉 cost의 경사를 구하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/r2jEHU9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 lost function 미분을 computing하는 과정을 살펴보겠습니다. 총 m개의 데이터가 있으니 m번의 loss를 구해서(m번의 loop) 계속 더해가고 마지막에 m으로 나누어 평균을 구하면 끝! loop가 많을 수록 computing시간은 길어지고 학습속도는 느려지게 된다. 따라서 loop문을 최소한으로 이용하기 위해 vectorizing을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Why Use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/q66KmrT.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위 그림과 같이, W 와 X 라는 배열이 있다고 가정\n",
    "  예를 들어 Z = W^T^x + b 라는 연산을 할 때, Vectorization이 되어 있지 않다면, for문을 x번 돌려서 Z를 계산하지만, Vectorization되어 있다면, 1번의 Vector계산으로 가능하다.\n",
    "\n",
    "* 하지만, 물론 CPU또는 GPU에서 지원하지 않는다면 공식을 어떻게 세우느냐와 관계 없이 실제 계산은 x번 반복하게 될 것이다. 그러나, 현대의 CPU는 SIMD(Single Instruction Multiple Data)를 지원한다.\n",
    "\n",
    "* SIMD란, 한번의 CPU/GPU 연산에 여러 개의 데이터를 넣을 수 있는 연산자(어셈블리)로 Vector 연산등을 위해 제공되고 있다.\n",
    "\n",
    "* 우리는 Python 에서 numpy를 통해 해당 연산자를 사용하는 연산을 할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/oIxTe89.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network를 일반적인 수식과 numpy를 이용한 vector 연산을 한 예시가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 . How to Use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/LNj4Ga9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림과 같은 경우에, vector v 를 e로 변화 시키고 싶다면, 기존 방식에서는 v vector 를 for loop을 돌며 하나씩 exp 연산을 하게 된다. 하지만, vector 연산인 np.exp(v)를 사용하면 하나의 for loop을 삭제 가능하다.\n",
    "비슷한 예시로, np.log(v), np.abs(v), np.maximum(v), v**2, 1/v 등의 연산이 제공된\n",
    "다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Logistic Regression의 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/SQnv6GX.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀는 다음과 같은 연산으로 표현된다.\n",
    "z^(1)^ = w ^T^x^(1)^ + b, a^(1)^ = , σ(z^(1)^)\n",
    "는 일반적인 for loop 대신에 vector 연산을 한다면 하나의 연산으로 표현 가능하다.\n",
    "\n",
    "z = w ^T^x^(1)^ = np.dot(w,T,x)\n",
    "z+b = np.dot(w,T,x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/DaVYSuW.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한, dz^(1)^ 또한 위 그림과 같이 Vector 연산 가능하며 최종적으로 아래와 같이 연산 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/zjbFuVO.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
