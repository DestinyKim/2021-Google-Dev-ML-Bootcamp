{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6702e3",
   "metadata": {},
   "source": [
    "## 1. Neural Networks Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d8978",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/fTzSvoh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d45462",
   "metadata": {},
   "source": [
    "* LR(Logistic Regression)과의 차이 파악\n",
    "* 딥러닝 정의 : 은닉층이 2개 이상일 경우에는 딥러닝이라고 함.\n",
    "* 딥러닝 특징1\n",
    "    - 노드들이 정확히 무엇을 추출하는지는 알 수 없음\n",
    "    - 각각의 노드가 정확히 무엇을 계산하는지 알 수 없음.\n",
    "    - 기계가 알아서 학습, 고수준의 학습 성능을 갖추고, 높은 결과를 예상\n",
    "    - 피쳐 엔지니어링이 필요 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d72aba",
   "metadata": {},
   "source": [
    "## 2. Neural Networks Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da83d6",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/wt08cUd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc469ef6",
   "metadata": {},
   "source": [
    "여기서 [i]의 표기는 i번째 layer라는 의미.\n",
    "dw와 같은 표기는 loss함수를 w로 미분한 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8446b3f",
   "metadata": {},
   "source": [
    "* 가장 좌측의 x1, x2, x3 부분은 Neural Network의 입력값으로 Input layer라고 함\n",
    "* 가장 우측의 하나의 노드로 구성되어 있는 y hat은 Output layer로 결과값을 출력함.\n",
    "* 중앙에 위치한 부분은 Hidden layer라고도 하며, 지도학습 과정 중, 입력값과 출력값이 명시적으로 존재하는 상황과 입력값으로 출력값을 학습하는 과정에서 값들이 명시적으로 보이지 않는 것을 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5937d",
   "metadata": {},
   "source": [
    "![](https://images.velog.io/images/jinseock95/post/696c827f-2ba1-403f-a1a2-b697b055c282/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcae2edf",
   "metadata": {},
   "source": [
    "* 보통 Input layer를 제외한 나머지 layer를 바탕으로 신경망을 정의하며, 위와 같은 경우는 2-layer Neural Network라고 한다.\n",
    "* 각 노드와 층에서는 LR의 계산 과정이 적용되는데, parameter인 w, b의 차원은 hidden layer와 노드의 수에 의해 결정된다.\n",
    "* 첫번째 layer에서 w, b는 각각 (4, 3), (4, 1)의 차원을 가지고 있고, 두번째 layer에서 w, b는 (1, 4), (1, 1)의 차원을 가지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69adfc0",
   "metadata": {},
   "source": [
    "## 3. Computing a Neural Network's Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37e358",
   "metadata": {},
   "source": [
    "* Input layer에 존재하는 입력값 중 하나인 x(i) 데이터가 2개의 layer를 거쳐 output layer에서 결과를 가지는 과정을 다음과 같은 계산 그래프로 정리할 수 있다.\n",
    "\n",
    "![](https://images.velog.io/images/jinseock95/post/0ad2a215-1cb3-41c9-8dbe-6926b7690211/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a97c0d",
   "metadata": {},
   "source": [
    "* 이것은 데이터 x(i) 하나에 대한 계산 그래프를 보여주는 것으로 데이터가 hidden layer의 노드를 거쳐 결과값이 산출되려면 위와 같은 과정이 노드별로 반복 및 중첩된다고 할 수 있다.\n",
    "* hidden layer의 모든 노드들에서 LR의 계산 과정을 적용하게 되면 각 노드별로 z값을 가지고, 모든 z값에 activation function을 적용하게 되면 4개의 a가 첫번째 hidden layer의 결과로 나오게 된다.\n",
    "* 하지만 위와 같은 하나의 데이터에 대한 모든 노드들에 대한 계산 과정을 4줄의 코드로 간단하게 줄일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff546b",
   "metadata": {},
   "source": [
    "![](https://images.velog.io/images/jinseock95/post/8361eac5-a9ac-4318-84d4-1cb51abfe0dc/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a4431",
   "metadata": {},
   "source": [
    "## 4. Vectorizing Across Multiple Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a2c9d",
   "metadata": {},
   "source": [
    "* 앞서 살펴본 계산 과정은 여전히 x(i) 하나의 데이터에 해당함으로 수많은 데이터의 계산 과정을 벡터화를 통해 보다 빠르게 적용할 수 있다.\n",
    "* m개의 입력 데이터가 존재할 때, x(1)부터 x(m)까지 iteration을 통해 각 데이터에 대한 결과값을 구할 수 있다.\n",
    "* 하지만 벡터화를 하게 되면 한 번에 계산이 가능함으로 훨씬 빠르게 결과를 구할 수 있으나, 이 때 차원을 잘 맞춰주는 것이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19705e79",
   "metadata": {},
   "source": [
    "![](https://media.vlpt.us/images/jinseock95/post/042a4e07-0b10-4634-9e4c-5587c1fdc594/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87d915",
   "metadata": {},
   "source": [
    "## 5. Explanation for Vectorized Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb6bc71",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/7tjLFN8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646738ef",
   "metadata": {},
   "source": [
    "* 1, 2, 3번째 샘플들에 대한 첫번째 레이어의 활성 결과들을 구한다.\n",
    "* 첫번째 레이어의 편향들은 정리를 위해서 깔끔하게 하기 위해서 0으로 만든다.\n",
    "* 첫번째 레이어의 가중치 행렬과 평행하게 쌓은 샘플 데이터의 행렬 곱 연산으로 한번에 z를 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74540ee",
   "metadata": {},
   "source": [
    "## 6. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6088c7",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa77b46",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/hQ4iGk0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03118f10",
   "metadata": {},
   "source": [
    "* 범위: 0 <= a <= 1\n",
    "* z값이 너무 작거나 크게 되면, 기울기(slope)가 0에 가까워짐으로 gradient descent가 매우 천천히 진행되어 성능이 좋지 않아, output layer에서 주로 사용되는 함수임\n",
    "* Sigmoid function의 장점은 미분값을 구할때 도함수가 a(1-a)이기 때문에 함수값을 이용할 수 있다는 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b9dc4",
   "metadata": {},
   "source": [
    "### Tanh(쌍곡선함수)\n",
    "![](https://i.imgur.com/mEN8XMl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc7da4",
   "metadata": {},
   "source": [
    "* 범위 : -1 <= a <= 1 \n",
    "* 대부분의 경우에 sigmoid보다 좋은 성능을 보이며 값의 범위가 -1과 +1 사이에 위치하게 되면서, 데이터 평균값이 0에 가깝게 유지되어 다음 층에서의 학습이 보다 수월함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69be4ad",
   "metadata": {},
   "source": [
    "### ReLU & Leaky ReLU\n",
    "![](https://i.imgur.com/LiPmEC1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e5331",
   "metadata": {},
   "source": [
    "* Relu: max(0, z)\n",
    "    * z가 0보다 클 때, 본래의 기울기를 가지는 특징으로 빠르게 gradient descent로 학습해 나갈 수 있기에, 가장 보편적으로 사용되는 함수임\n",
    "\n",
    "* Leaky Relu\n",
    "    * Relu가 0의 값을 가질 때, 성능이 저하되는 것을 방지하기 위해 개선한 함수임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a25c4",
   "metadata": {},
   "source": [
    "activation function이 중요한 이유는, 기울기(slope)를 통해 global optimum을 찾아가는 gradient descent가 반복되는 과정에서, 기울기를 반환해주는 activation function의 종류에 따라, 그리고 데이터 특징에 따라 결과가 달라지기 때문에 최적의 결과를 얻기 위해서는 다양한 접근이 필요함\n",
    "\n",
    "![](https://media.vlpt.us/images/jinseock95/post/11230cb8-9a01-4a04-a2fd-52e509ab6223/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ba8a6",
   "metadata": {},
   "source": [
    "## 7. Why do you need Non-Linear Activation Functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02731d",
   "metadata": {},
   "source": [
    "![](https://media.vlpt.us/images/jinseock95/post/f54a2962-fb48-4f39-a5b1-8ae46565a8af/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69273631",
   "metadata": {},
   "source": [
    "* Activation function이 non-linear이어야 하는 이유는 만약 Activation function이 선형이면 이는 activation function이 없는 것 과 같다. 단순 k배에 상수를 더하는 것은 w와 b로 해줄 수 있기 때문이다. 그러므로 위에서 볼 수 있듯이 input 과 output은 단순한 선형관계가 되어, 선형회귀가 되어버린다. 따라서 여러 층의 layer도 필요가 없어진다. 따라서 이와 같은 이유로 Activation function은 non-linear를 쓰는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0df2d8",
   "metadata": {},
   "source": [
    "## 8. Derivatives of Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6562c70e",
   "metadata": {},
   "source": [
    "* Neural Network의 학습을 위해서 Backpropagation을 적용하기 위해서는 Activation function의 derivative를 구해야함\n",
    "* 각 Activation function의 그래프에서 특정 지점에서의 slope가 Activation function의 derivative라고 이해 할 수 있다.\n",
    "    * sigmoid : g’(z) = a(1 - a) 이며, 0과 1에 가까워질수록 기울기값이 작아짐을 알 수 있음\n",
    "\n",
    "    * tanh: g’(z) = 1 - a^2 이며, -1과 1에 가까워질수록 기울기값이 작아짐을 알 수 있음\n",
    "\n",
    "    * Relu: g’(z) = 0 if z < 0, 1 if z >= 0 이며, leaky relu일 때는 0이 아닌 0.01을 반환해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f6396",
   "metadata": {},
   "source": [
    "## 9. Gradient Descent for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a0070",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/vytoMqv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0c8a8",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/CBPANqY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60fc62",
   "metadata": {},
   "source": [
    "* 2-layer neural network의 gradient descent가 반복적으로 적용되면서 parameter w와 b가 업데이트되는 과정은 위의 그림과 같다.\n",
    "* 기본은 dz를 구하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ff254",
   "metadata": {},
   "source": [
    "## 11. Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63ec8b",
   "metadata": {},
   "source": [
    "* neural network를 설계할 때, 적절한 parameter를 찾기 위한 반복적인 과정을 거치는데, 초기 parameter를 임의로 설정하는 것이 굉장히 중요함\n",
    "    * neural network에서 초기 weight를 0으로 설정하게 되면 한 layer의 모든 노드에서 같은 결과를 반환하기 때문에 모두 같은 방식으로 update가 진행되어 의미없는 결과가 나오게 됨(아래 그림 참고)\n",
    "    ![](https://i.imgur.com/BlA8WMv.png)\n",
    "    * 이렇게 모든 노드에서 같은 연산을 수행하게 되어 똑같은 결과를 가지게 되는 것을 '대칭(symmetry)'이라고 하는데, weight를 랜덤으로 설정함으로써 symmetry breaking 할 수 있게 됨\n",
    "    * 이 때, 랜덤으로 설정하는 weight 값이 매우 크게 되면 기울기가 0으로 소실되어 버리기 때문에 통상 매우 작은 값으로 설정해줌\n",
    "    ![](https://i.imgur.com/zJe45lD.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
