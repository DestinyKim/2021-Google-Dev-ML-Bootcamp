{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Introduction to gradients and automatic differentiation.ipynb","provenance":[],"authorship_tag":"ABX9TyM28gmrwVXTjfNDas0U+uX5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"HmQPFjNhVF7B","executionInfo":{"status":"ok","timestamp":1629694022169,"user_tz":-540,"elapsed":1945,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4Msb4b4VSgI","executionInfo":{"status":"ok","timestamp":1629694119066,"user_tz":-540,"elapsed":636,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}}},"source":["# tf.GradientTape : 자동 미분(주어진 입력 변수에 대한 연산의 Gradient를 계산하는 것)\n","\n","x = tf.Variable(3.0)\n","\n","with tf.GradientTape() as tape :\n","  y = x**2"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-R_xQ7hRVsi0","executionInfo":{"status":"ok","timestamp":1629694145408,"user_tz":-540,"elapsed":269,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}},"outputId":"81ae30b6-3036-4b03-c92d-d5a8578ebe68"},"source":["dy_dx = tape.gradient(y, x)\n","dy_dx.numpy()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.0"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"JQdQ10LdVy-n","executionInfo":{"status":"ok","timestamp":1629694284659,"user_tz":-540,"elapsed":239,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}}},"source":["w = tf.Variable(tf.random.normal((3, 2)), name = 'w')\n","b = tf.Variable(tf.zeros(2, dtype = tf.float32), name = 'b')\n","x = [[1., 2., 3.]]\n","\n","with tf.GradientTape(persistent=True) as tape:\n","  y = x @ w + b\n","  loss = tf.reduce_mean(y**2)\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TVhQwznXWUY-"},"source":["두 변수 모두에 대해 loss의 그래디언트를 가져오려면, 두 변수를 gradient 메서드에 소스로 전달할 수 있습니다. 테이프는 소스가 전달되는 방식에 대해 융통성이 있으며 목록 또는 사전의 중첩된 조합을 허용하고 같은 방식으로 구조화된 그래디언트를 반환합니다"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDchZAANWcMh","executionInfo":{"status":"ok","timestamp":1629694364096,"user_tz":-540,"elapsed":257,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}},"outputId":"67a4ad50-220b-45cc-ffc6-c23ad4a1b924"},"source":["[dl_dw, dl_db] = tape.gradient(loss, [w, b]) # 각 소스에 대한 그래디언트는 소스의 형상을 갖는다\n","\n","print(w.shape)\n","print(dl_dw.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(3, 2)\n","(3, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lpLnT3BXWoV0","executionInfo":{"status":"ok","timestamp":1629694449985,"user_tz":-540,"elapsed":248,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}},"outputId":"accf6a4d-3ec0-464e-dc20-3476efdbdf43"},"source":["# 변수에 dictionary를 전달하는 그래디언트 계산\n","my_vars = {'w':w, 'b':b}\n","\n","grad = tape.gradient(loss, my_vars)\n","grad['b']"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 4.1267757 , -0.20443402], dtype=float32)>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"VdRXoJPGW9bd","executionInfo":{"status":"ok","timestamp":1629695121958,"user_tz":-540,"elapsed":242,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}}},"source":["# 모델에 대한 그래디언트\n","\n","layer = tf.keras.layers.Dense(2, activation = 'relu')\n","x = tf.constant([[1., 2., 3.]])\n","\n","with tf.GradientTape() as tape:\n","  # Forward pass\n","  y = layer(x)\n","  loss = tf.reduce_mean(y**2)\n","\n","# Calculate gradients with respect to every trainable variable\n","grad = tape.gradient(loss, layer.trainable_variables)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CB0l76DiZe3l","executionInfo":{"status":"ok","timestamp":1629695160279,"user_tz":-540,"elapsed":241,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}},"outputId":"49759169-8fc8-4f6f-efc3-303fb8df5e5a"},"source":["for var, g in zip(layer.trainable_variables, grad):\n","  print(f'{var.name}, shape:{g.shape}')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["dense/kernel:0, shape:(3, 2)\n","dense/bias:0, shape:(2,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t_iBEcjBZq16"},"source":["### tape의 감시 대상 제어하기\n","\n","기본 동작은 훈련 가능한 tf.Variable에 액세스한 후 모든 연산을 기록하는 것. 그 이유는 다음과 같다.\n","\n","* 테이프는 역방향 패스의 그래디언트를 계산하기 위해 정방향 패스에 기록할 연산을 알아야 한다.\n","* 테이프는 중간 출력에 대한 참조를 보유하므로 불필요한 연산을 기록하지 않는다.\n","* 가장 일반적인 사용 사례는 모든 모델의 훈련 가능한 변수에 대해 손실의 그래디언트를 계산하는 것이다.\n","  \n","예를 들어, 다음은 tf.Tensor가 기본적으로 \"감시\"되지 않고 tf.Variable을 훈련할 수 없기 때문에 그래디언트를 계산하지 못한다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_jpX0sg1aTV7","executionInfo":{"status":"ok","timestamp":1629695547061,"user_tz":-540,"elapsed":236,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}},"outputId":"f8918df6-d5e4-4ba4-d98a-473f38390b92"},"source":["# A trainable variable\n","x0 = tf.Variable(3.0, name = 'x0')\n","# Not trainable\n","x1 = tf.Variable(3.0, name = 'x1', trainable=False)\n","# Not a Variable: A variable + tensor returns a tensor.\n","x2 = tf.Variable(2.0, name = 'x2') + 1.0\n","# Not a Variable\n","x3 = tf.constant(3.0, name = 'x3')\n","\n","with tf.GradientTape() as tape:\n","  y = (x0**2) + (x1**2) + (x2**2)\n","\n","grad = tape.gradient(y, [x0, x1, x2, x3])\n","\n","for g in grad:\n","  print(g)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tf.Tensor(6.0, shape=(), dtype=float32)\n","None\n","None\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4gabTA1bbJOX","executionInfo":{"status":"ok","timestamp":1629695588558,"user_tz":-540,"elapsed":238,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}},"outputId":"88f35a75-87fb-46b2-f619-091c8f7b14f6"},"source":["# GradientTape.watched_variables : tape에서 감시중인 변수를 나열\n","\n","[var.name for var in tape.watched_variables()]"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['x0:0']"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z_YvvD7EbTZy","executionInfo":{"status":"ok","timestamp":1629695689648,"user_tz":-540,"elapsed":248,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}},"outputId":"61ea5e0e-3c5a-44c9-97ef-d717a0772a89"},"source":["# tf.GradientTape는 사용자가 감시 대상 또는 감시 예외 대상을 제어할 수 있는 후크를 제공\n","# tf.Tensor에 대한 그래디언트를 기록하려면 GradientTape.watch(x)를 호출\n","\n","x = tf.constant(3.0)\n","with tf.GradientTape() as tape:\n","  tape.watch(x)\n","  y = x**2\n","\n","# dy = 2x * dx\n","dy_dx = tape.gradient(y, x)\n","print(dy_dx.numpy())"],"execution_count":13,"outputs":[{"output_type":"stream","text":["6.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HRCcOlwWbr_j"},"source":["반대로, 모든 tf.Variables을 감시하는 기본 동작을 비활성화하려면, 그래디언트 테이프를 만들 때 watch_accessed_variables=False를 설정한다. 이 계산은 두 가지 변수를 사용하지만, 변수 중 하나의 그래디언트만 연결한다."]},{"cell_type":"code","metadata":{"id":"TvdVcytab2DN","executionInfo":{"status":"ok","timestamp":1629695834414,"user_tz":-540,"elapsed":248,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}}},"source":["x0 = tf.Variable(0.0)\n","x1 = tf.Variable(10.0)\n","\n","with tf.GradientTape(watch_accessed_variables=False) as tape:\n","  tape.watch(x1)\n","  y0 = tf.math.sin(x0)\n","  y1 = tf.nn.softplus(x1)\n","  y = y0 + y1\n","  ys = tf.reduce_sum(y)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qBB_I6MrcPbH"},"source":["x0에서 GradientTape.watch가 호출되지 않았으므로 이에 대한 그래디언트가 계산되지 않는다.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikCi0rBWcXT4","executionInfo":{"status":"ok","timestamp":1629695944347,"user_tz":-540,"elapsed":236,"user":{"displayName":"Nameun Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1z_nO41g88UbevUF_QsRyy00S5BlTgbMVh0FC9Q=s64","userId":"06338012911543066269"}},"outputId":"f80d0f99-ff11-46ab-bd8b-a4018acb660c"},"source":["# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n","grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n","\n","print('dy/dx0', grad['x0'])\n","print('dy/dx1', grad['x1'].numpy())"],"execution_count":15,"outputs":[{"output_type":"stream","text":["dy/dx0 None\n","dy/dx1 0.9999546\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BNJjbQmUcqLm"},"source":[""],"execution_count":null,"outputs":[]}]}